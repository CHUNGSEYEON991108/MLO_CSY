from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
from datetime import datetime
import mlflow
import mlflow.sklearn

def plot_roc_curve(y_true, y_pred_proba, save_path=None):
    """
    Plot ROC curve
    
    Args:
        y_true: True labels
        y_pred_proba: Predicted probabilities
        save_path: Path to save the plot
    """
    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
    roc_auc = auc(fpr, tpr)
    
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, 
             label=f'ROC curve (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc="lower right")
    
    if save_path:
        plt.savefig(save_path)
        print(f"ROC ì»¤ë¸Œê°€ {save_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    plt.close()
    return roc_auc

def evaluate_model(model, X, y):
    """
    Evaluate model performance using various metrics
    
    Args:
        model: Trained model
        X: Features
        y: True labels
        
    Returns:
        dict: Dictionary containing evaluation metrics
    """
    y_pred = model.predict(X)
    y_pred_proba = model.predict_proba(X)[:, 1]
    
    metrics = {
        'accuracy': accuracy_score(y, y_pred),
        'precision': precision_score(y, y_pred),
        'recall': recall_score(y, y_pred),
        'f1': f1_score(y, y_pred),
        'roc_auc': plot_roc_curve(y, y_pred_proba)
    }
    
    return metrics

def train_model(data, output_dir=None, experiment_name="ì±„ë¬´ë¶ˆì´í–‰ì˜ˆì¸¡"):
    """
    Train a random forest classifier with MLflow tracking
    
    Args:
        data (pandas.DataFrame): Training data with features and target
        output_dir (str, optional): Directory to save plots
        experiment_name (str): Name of the MLflow experiment
        
    Returns:
        RandomForestClassifier: Trained model
    """
    # MLflow ì‹¤í—˜ ì„¤ì •
    mlflow.set_experiment(experiment_name)
    
    # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
    X = data.drop(['UID', 'ì±„ë¬´ ë¶ˆì´í–‰ ì—¬ë¶€'], axis=1)
    y = data['ì±„ë¬´ ë¶ˆì´í–‰ ì—¬ë¶€']
    
    # ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬
    categorical_cols = X.select_dtypes(include=['object']).columns
    X = pd.get_dummies(X, columns=categorical_cols)
    
    # í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• 
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # MLflowë¡œ ì‹¤í—˜ ì¶”ì 
    with mlflow.start_run():
        # ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
        model_params = {
            'n_estimators': 100,
            'random_state': 42
        }
        
        # ì‹¤í—˜ íŒŒë¼ë¯¸í„° ê¸°ë¡ (ëª¨ë¸ ë° ë°ì´í„° ë¶„í•  íŒŒë¼ë¯¸í„° í¬í•¨)
        mlflow.log_params({
            **model_params,
            'test_size': 0.2
        })
        
        # ëª¨ë¸ í•™ìŠµ
        model = RandomForestClassifier(**model_params)
        model.fit(X_train, y_train)
        
        # êµì°¨ ê²€ì¦ ìˆ˜í–‰
        cv_scores = cross_val_score(model, X, y, cv=5, scoring='roc_auc')
        print(f"\nğŸ“Š êµì°¨ ê²€ì¦ ROC-AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
        
        # ê²€ì¦ ë°ì´í„°ë¡œ ëª¨ë¸ í‰ê°€
        metrics = evaluate_model(model, X_val, y_val)
        
        # ë©”íŠ¸ë¦­ ê¸°ë¡
        mlflow.log_metrics({
            'cv_roc_auc_mean': cv_scores.mean(),
            'cv_roc_auc_std': cv_scores.std(),
            'val_accuracy': metrics['accuracy'],
            'val_precision': metrics['precision'],
            'val_recall': metrics['recall'],
            'val_f1': metrics['f1'],
            'val_roc_auc': metrics['roc_auc']
        })
        
        # ROC ì»¤ë¸Œ ê·¸ë¦¬ê¸° ë° ì €ì¥
        if output_dir:
            version = datetime.now().strftime("%Y%m%d_%H%M")
            roc_path = os.path.join(output_dir, f'roc_curve_v{version}.png')
            y_pred_proba = model.predict_proba(X_val)[:, 1]
            plot_roc_curve(y_val, y_pred_proba, roc_path)
            mlflow.log_artifact(roc_path)
        
        # ëª¨ë¸ ì €ì¥
        mlflow.sklearn.log_model(model, "random_forest_model")
    
    return model

def predict(model, X):
    """
    Make predictions using the trained model
    
    Args:
        model: Trained model
        X (array-like): Features to predict
        
    Returns:
        tuple: (Predicted labels, Predicted probabilities)
    """
    y_pred = model.predict(X)
    y_pred_proba = model.predict_proba(X)[:, 1]
    
    return y_pred, y_pred_proba

def predict_and_save(model, test_data, output_path):
    """
    Make predictions and save them to a file
    
    Args:
        model: Trained model
        test_data (pandas.DataFrame): Test data
        output_path (str): Path to save predictions
    """
    print("\nğŸ”„ ì˜ˆì¸¡ ì¤‘...")
    
    # íŠ¹ì„± ì¤€ë¹„
    X_test = test_data.drop(['UID'], axis=1)
    categorical_cols = X_test.select_dtypes(include=['object']).columns
    X_test = pd.get_dummies(X_test, columns=categorical_cols)
    
    # ì˜ˆì¸¡
    _, y_pred_proba = predict(model, X_test)
    
    # ê²°ê³¼ ì €ì¥
    version = datetime.now().strftime("%Y%m%d_%H%M")
    output_dir = os.path.dirname(output_path)
    output_name = os.path.splitext(os.path.basename(output_path))[0]
    versioned_path = os.path.join(output_dir, f'{output_name}_v{version}.csv')
    
    submission = pd.DataFrame({
        'UID': test_data['UID'],
        'ì±„ë¬´ ë¶ˆì´í–‰ í™•ë¥ ': y_pred_proba
    })
    submission.to_csv(versioned_path, index=False)
    
    # MLflowì— ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
    with mlflow.start_run(nested=True):
        mlflow.log_artifact(versioned_path)
    
    print(f"\nâœ… ì˜ˆì¸¡ ê²°ê³¼ê°€ {versioned_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.") 